\label{sec:ch5}

Our ultimate goal in this dissertation was to make drone cameras easier to use and more expressive.
To make progress towards this goal, we used the principles of trajectory optimization to express high-level drone behaviors, and to solve for low-level drone actions.
We also used domain knowledge to design specialized algorithms that were tailored to particular camera tasks.
In other words, we did not attempt to solve general drone intelligence.
Instead, we used domain-specific methods to make two targeted creative tasks -- drone cinematography and drone 3D scanning -- easier for users to specify.

In Chapter \ref{sec:ch2}, we introduced drone cinematography as a research problem.
Specifically, we introduced a set of design principles for quadrotor shot planning tools, and based on these principles, we built a tool for designing quadrotor camera shots.
We added four components to existing quadrotor mission planning tools: (1) visual shot design; (2) virtual preview; (3) precise timing control; and (4) visual feasibility feedback. 
To support our tool, we introduced a physical model for quadrotor cameras, and we derived an algorithm for generating camera trajectories that agree with our physical model.
Using our tool, both novices and expert users designed compelling shots that would be challenging to create otherwise.
We successfully and autonomously captured all shots with reasonable accuracy on a real quadrotor camera platform.

In Chapter \ref{sec:ch3}, we extended the capabilities of our shot planning tool, enabling it to automatically generate dynamically feasible trajectories for quadrotor cameras.
We analyzed the dynamics of a quadrotor along a fixed path, and we found that the quadrotor's velocities and control forces are fully determined by its progress curve along the path.
This insight lead us to a fast and user-friendly algorithm for generating dynamically feasible trajectories.
We implemented our algorithm in our shot planning tool, and we ran performance benchmarks on a dataset of infeasible quadrotor camera trajectories.
We found that our approach is between 25$\times$ and 180$\times$ faster than a spacetime constraints approach.
We successfully captured real video footage using the trajectories generated by our algorithm.
We showed that the resulting videos are faithful to virtual shot previews, even when the trajectories being executed are at our quadrotor's physical limits.

In Chapter \ref{sec:ch4}, we turned our attention from cinematography to 3D scanning.
We proposed an intuitive coverage model for aerial 3D scanning, and we made the observation that our model is submodular.
We leveraged submodularity to develop a computationally efficient method for generating scanning trajectories, that reasons jointly about coverage rewards and travel costs.
We evaluated our method by using it to scan three large real-world scenes, and a scene in a photorealistic video game simulator.
We found that our method results in quantitatively higher-quality 3D reconstructions than baseline methods, both geometrically and visually.

\section{Future Work}

In the future, consumer robotics will play a critical role in a wide range of creative tasks. 
We saw concrete examples of this general idea in Chapters \ref{sec:ch2} and \ref{sec:ch3}, where we used drones to help us create ambitious cinematography shots that would be challenging or impossible to obtain otherwise.
But the work in this dissertation is far from the last word on this topic, and there is plenty of exciting work left to do.
For example, drones can now autonomously film a person skiing down a mountain [CITE].
\emph{Activity forecasting} methods [CITE] and computational models of human aesthetic preferences [CITE] might soon be used to plan better-looking shots in real-time.
Filming with a team of drones [CITE] might soon enable new kinds of \emph{bullet-time} camera effects [CITE] when filming action sports in unstructured environments. Computational models of \emph{social saliency} [CITE] could enable drones to capture outdoor social events autonomously and unintrusively.
Advances in drone hardware could enable dramatic new types of \emph{hyper-lapse} photography [CITE].

We also saw examples of robot-enabled creativity support in Chapter \ref{sec:ch4}, where we used drones to generate high-quality virtual models of an environment, e.g., for use in a video game or virtual reality experience.
More generally, drones\ will enable us to capture the physical world with unprecedented coverage and scale.
Cooperating teams of drones might soon be used to scan large scenes very quickly.
New kinds of drones might be used to scan indoor environments at unprecedented levels of detail. Cooperating drone cameras and drone lights might be used to capture richer material representations.

The idea of using robots to help us create is much bigger than the examples we've seen in this dissertation, because there are so many creative tasks that we would like help with.
For example, drone lights are now being used to create swarming 3D displays [CITE].
Thinking about this new type of computational display immediately leads us to a host of open questions.
How should we specify appropriate goals for a swarming 3D display, and how would we compile an animation into a swarm of collision-free drone trajectories?

Moving beyond aerial drones, there are many creative tasks at home that household chore robots might help us with.
A particularly fascinating example is cooking. 
How should we specify goals for a cooking robot, and how might we convert these goals into long-range robot trajectories?
Similar open questions arise in a variety of other creative tasks around the house, e.g., assembling and re-arranging furniture.

Perhaps an even greater degree of situational awareness is required outside the home, where we would like help from robots to build on-demand public infrastructure like roads and bridges, e.g., in disaster relief scenarios.

Across a wide range of domains, we want robots to help us \emph{create}.
The resulting computational problems are hard, but the language of trajectory optimization gives us the tools we need we make progress.
And we should try to make progress now, because the next generation of consumer-facing robots is just around the corner.
